{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatimazain118/IoT-project/blob/main/Covid_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RzCryQyOt80"
      },
      "source": [
        "import numpy as np \n",
        "#import random\n",
        "#import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "#from statsmodels.tsa.seasonal import seasonal_decompose \n",
        "#import warnings \n",
        "from pmdarima import auto_arima \n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.forecasting.stl import STLForecast\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import xlrd\n",
        "import pdb, random\n",
        "\n",
        "import xlrd, xlwt\n",
        "from xlwt import Workbook as wbb\n",
        "from numpy import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_store(lt,lp,tt):\n",
        "    fn=open(\"/root/Desktop/result-forecast2.xlsx\", 'a')\n",
        "    fn.write('\\n'+str(tt))                             \n",
        "    fn.write(\"\\nAccuracy,\"+str((accuracy_score(lt,lp))*100))\n",
        "    fn.write(\"\\nAdj. Rand Index,\"+str(metrics.adjusted_rand_score(lt,lp)))\n",
        "    fn.write(\"\\nHomogeneity,\"+str(metrics.homogeneity_score(lt,lp)))\n",
        "    fn.write(\"\\nCompleteness,\"+str(metrics.completeness_score(lt,lp)))\n",
        "    fn.write(\"\\nV-measure,\"+str(metrics.v_measure_score(lt,lp)))\n",
        "    fn.write(\"\\nFowlkes Mallows,\"+str(metrics.fowlkes_mallows_score(lt,lp)))\n",
        "    \n",
        "    \n",
        "    fn.close()\n",
        "    \n",
        "\n",
        "def score_cal():\n",
        "    fp_sc = \"/root/Desktop/forecast400-pattern3.xlsx\"\n",
        "    fn = open(\"/root/Desktop/score2.txt\",'a')\n",
        "    wb_sc = xlrd.open_workbook(fp_sc)\n",
        "    sheet1 = wb_sc.sheet_by_index(0)\n",
        "    score_l1 = []\n",
        "    all_pattern1 = []\n",
        "    for j in range(sheet1.nrows):\n",
        "        ll = []\n",
        "        for i in range(sheet1.ncols):\n",
        "            x1=sheet1.cell_value(j,i)\n",
        "            ll.append(x1)\n",
        "        #pdb.set_trace()\n",
        "        score =4/sqrt(matrix([-0.002,-0.002,-0.002,0.2,0.2,0.2])*matrix(ll).T)\n",
        "        score = score.tolist()[0][0]\n",
        "        ll.append(score)\n",
        "        print(score)\n",
        "        if score <0.38:\n",
        "            fs = 0\n",
        "        else:\n",
        "            fs = 1\n",
        "\n",
        "        score_l1.append(fs)\n",
        "        all_pattern1.append(ll)    \n",
        "        fn.write(\"\\n\"+str(score))\n",
        "    \n",
        "    \n",
        "    print(all_pattern1)\n",
        "    #print(score_l1)\n",
        "    #pdb.set_trace()\n",
        "    X = array(all_pattern1)\n",
        "    ygt=[]\n",
        "    for ii in range(199):\n",
        "        ygt.append(0)\n",
        "    \n",
        "    for ii in range(200):\n",
        "        ygt.append(1)\n",
        "    ygt = array(ygt)\n",
        "    \n",
        "    #y = array(score_l1)\n",
        "    #preparing dataset\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 10)\n",
        "    \n",
        "    #print (len(X_train))\n",
        "    #print (len(X_test))\n",
        "    #print (len(y_train))\n",
        "    #print (len(y_test))\n",
        "    \n",
        "    #print(y_test)\n",
        "    #print(X_test)\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    \n",
        "    labels_true = ygt\n",
        "\n",
        "    ##### clustering##############\n",
        "    estimators = [('COVID 3 Types', KMeans(n_clusters=3, random_state=4)), \n",
        "                  ('COVID 2 Types', KMeans(n_clusters=2, random_state=4)),\n",
        "              ('COVID 2 Types random initialization ', KMeans(n_clusters=2, n_init=1, init='random')),\n",
        "              ('COVID 5 Types', KMeans(n_clusters=5, random_state=4)),\n",
        "              ('COVID 10 Types', KMeans(n_clusters=10, random_state=4)),\n",
        "              ('COVID 20 Types', KMeans(n_clusters=20, random_state=4)),\n",
        "              ('COVID 30 Types', KMeans(n_clusters=30, random_state=4)),\n",
        "              ('COVID 40 Types', KMeans(n_clusters=40, random_state=4)),\n",
        "              ('COVID 50 Types', KMeans(n_clusters=50, random_state=4)),\n",
        "              ('COVID 60 Types', KMeans(n_clusters=60, random_state=4)),\n",
        "              ('COVID 70 Types', KMeans(n_clusters=70, random_state=4)),\n",
        "              ('COVID 80 Types', KMeans(n_clusters=80, random_state=4)),\n",
        "              ('COVID 90 Types', KMeans(n_clusters=90, random_state=4)),\n",
        "              ('COVID 100 Types', KMeans(n_clusters=100, random_state=4)),\n",
        "              ('COVID 3 Types through Birch', Birch(n_clusters=None)),\n",
        "              ('COVID 2 Types through Birch', Birch(n_clusters=2)),\n",
        "              ('COVID 5 Types through Birch', Birch(n_clusters=5)),\n",
        "              ('COVID 10 Types through Birch', Birch(n_clusters=10)),\n",
        "              ('COVID 20 Types through Birch', Birch(n_clusters=20)),\n",
        "              ('COVID 30 Types through Birch', Birch(n_clusters=30)),\n",
        "              ('COVID 40 Types through Birch', Birch(n_clusters=40)),\n",
        "              ('COVID 50 Types through Birch', Birch(n_clusters=50)),\n",
        "              ('COVID 60 Types through Birch', Birch(n_clusters=60)),\n",
        "              ('COVID 70 Types through Birch', Birch(n_clusters=70)),\n",
        "              ('COVID 80 Types through Birch', Birch(n_clusters=80)),\n",
        "              ('COVID 90 Types through Birch', Birch(n_clusters=90)),\n",
        "              ('COVID 100 Types through Birch', Birch(n_clusters=100))]\n",
        "\n",
        "    fignum = 1\n",
        "    titles = ['3 Types of COVID patient Using K-Means',\n",
        "              '2 Types of COVID patient Using K-Means', \n",
        "              '2 Types of COVID patient Using K-Means(random initialization)',\n",
        "              '5 Types of COVID patient Using K-Means',\n",
        "              '10 Types of COVID patient Using K-Means',\n",
        "              '20 Types of COVID patient Using K-Means',\n",
        "              '30 Types of COVID patient Using K-Means',\n",
        "              '40 Types of COVID patient Using K-Means',\n",
        "              '50 Types of COVID patient Using K-Means',\n",
        "              '60 Types of COVID patient Using K-Means',\n",
        "              '70 Types of COVID patient Using K-Means',\n",
        "              '80 Types of COVID patient Using K-Means',\n",
        "              '90 Types of COVID patient Using K-Means',\n",
        "              '100 Types of COVID patient Using K-Means',\n",
        "              '3 Types of COVID patient Using Birch',\n",
        "              '2 Types of COVID patient Using Birch',\n",
        "              '5 Types of COVID patient Using Birch',\n",
        "              '10 Types of COVID patient Using Birch',\n",
        "              '20 Types of COVID patient Using Birch',\n",
        "              '30 Types of COVID patient Using Birch',\n",
        "              '40 Types of COVID patient Using Birch',\n",
        "              '50 Types of COVID patient Using Birch',\n",
        "              '60 Types of COVID patient Using Birch',\n",
        "              '70 Types of COVID patient Using Birch',\n",
        "              '80 Types of COVID patient Using Birch',\n",
        "              '90 Types of COVID patient Using Birch',\n",
        "              '100 Types of COVID patient Using Birch']\n",
        "    titles_num=['3','2','2','5','10','20','30','40','50','60','70','80','90','100',\n",
        "                'B3','B2','B5','B10','B20','B30','B40','B50','B60','B70','B80','B90','B100',]\n",
        "    for name, ward in estimators:\n",
        "        fig = plt.figure(fignum, figsize=(4, 3))\n",
        "        ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=40, azim=120)\n",
        "        ward.fit(X)\n",
        "        ##pdb.set_trace()\n",
        "        labels = ward.labels_\n",
        "        #result display and store#\n",
        "        text_data = titles_num[fignum - 1]\n",
        "        result_store(labels_true, labels, text_data)\n",
        "        \n",
        "        ##  DISPLAY  ###\n",
        "    \n",
        "        ax.scatter(X[:, 0], X[:, 2], X[:, 6],\n",
        "                   c=labels.astype(np.float), edgecolor='k')\n",
        "    \n",
        "        ax.w_xaxis.set_ticklabels([])\n",
        "        ax.w_yaxis.set_ticklabels([])\n",
        "        ax.w_zaxis.set_ticklabels([])\n",
        "        ax.set_xlabel('Std. Deviation')\n",
        "        ax.set_ylabel('Amplitude')\n",
        "        ax.set_zlabel('Score')\n",
        "        ax.set_title(titles[fignum - 1])\n",
        "        ax.dist = 12\n",
        "        fignum = fignum + 1\n",
        "    fig.show()      \n",
        "    # Plot the ground truth\n",
        "   ####### evalute#################\n",
        "    \n",
        "def dataprocessing(file_path,pn):\n",
        "    \n",
        "    wb = xlrd.open_workbook(file_path)\n",
        "    sheet = wb.sheet_by_index(pn)\n",
        "    sheet.cell_value(0,0)\n",
        "    temp_list = []\n",
        "    pulse_list = []\n",
        "    oxy_list = []\n",
        "    for j in range(sheet.nrows):\n",
        "        for i in range(sheet.ncols):\n",
        "            x1=sheet.cell_value(j,i)\n",
        "            if x1 =='Tempareture':\n",
        "                flag_temp = True\n",
        "                flag_pulse = False\n",
        "                flag_oxy = False\n",
        "            elif x1 == 'Pulse rate':\n",
        "                flag_temp = False\n",
        "                flag_pulse = True\n",
        "                flag_oxy = False\n",
        "            elif x1 =='Oxygen saturation':\n",
        "                flag_temp = False\n",
        "                flag_pulse = False\n",
        "                flag_oxy = True\n",
        "                \n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "            if (flag_temp):\n",
        "                if type(x1) is float:\n",
        "                    temp_list.append(x1)\n",
        "                else:\n",
        "                    pass\n",
        "            elif (flag_pulse):\n",
        "                if type(x1) is float:\n",
        "                    pulse_list.append(x1)\n",
        "                else:pass\n",
        "            elif (flag_oxy):\n",
        "                if type(x1) is float:\n",
        "                    oxy_list.append(x1)\n",
        "                else:pass\n",
        "            else: pass\n",
        "    #pdb.set_trace()\n",
        "    \n",
        "    return (temp_list, pulse_list, oxy_list)\n",
        "def main():\n",
        "    fp=\"/root/Desktop/RA/new_symptoms1.xlsx\"\n",
        "    p_number = 399\n",
        "    p_all_fr = []\n",
        "    p_all_org = []\n",
        "    for pn in list(range(p_number)):\n",
        "        data = dataprocessing(fp, pn)\n",
        "        #temp_list = data[0]\n",
        "        #pulse_list = data[1]\n",
        "        #oxy_list = data[2]\n",
        "        symp_fr = []\n",
        "        symp_org = []\n",
        "        arima_rmse=0\n",
        "        arima_r2=0\n",
        "        arima_mae=0\n",
        "        es_rmse=0\n",
        "        es_r2=0\n",
        "        es_mae=0\n",
        "        ols_rmse=0\n",
        "        ols_r2=0\n",
        "        ols_mae=0\n",
        "        sdl_rmse=0\n",
        "        sdl_r2=0\n",
        "        sdl_mae=0\n",
        "        varma_rmse=0\n",
        "        varma_r2=0\n",
        "        varma_mae=0\n",
        "\n",
        "        for values in data:\n",
        "        \n",
        "            #values = np.matrix(values)\n",
        "            train, test = values[:100], values[100:]\n",
        "    \n",
        "            #model = auto_arima(train, m=4, seasonal = True)          \n",
        "            \n",
        "            \n",
        "            \n",
        "           # model_data=arima_model.summary()\n",
        "            #pdb.set_trace()\n",
        "            ###############fit SARIMAX###############\n",
        "            model = sm.tsa.SARIMAX(train, order=(1,1,1),\n",
        "                             seasonal_order=(0,1,2,4))\n",
        "            #SARIMAX(train,order = arima_model.order, seasonal_order =arima_model.seasonal_order)\n",
        "            #result = model.fit()\n",
        "\n",
        "            start1= len(train)\n",
        "            end1 =(len(train)-1)+len(train)*2\n",
        "            arima_fr=result.predict(start = start1,end = end1,typ = 'levels')\n",
        "            \n",
        "            arima_mse=mean_squared_error(test,arima_fr[:len(test)])\n",
        "            arima_rmse += sqrt(arima_mse)\n",
        "            arima_r2 += r2_score(test,arima_fr[:len(test)]) #, multioutput='variance_weighted')\n",
        "            arima_mae += mean_absolute_error(test,arima_fr[:len(test)])\n",
        "            #####################exponential smoothing############################################\n",
        "            #pdb.set_trace()    \n",
        "            model = SimpleExpSmoothing(np.asarray(train))\n",
        "            \n",
        "            fit1 = model.fit()\n",
        "            es_fr = fit1.forecast(len(values)+1)\n",
        "            \n",
        "            es_mse = mean_squared_error(test, es_fr[:len(test)])\n",
        "            es_rmse += sqrt(es_mse)\n",
        "            es_r2 += r2_score(test, es_fr[:len(test)],  multioutput='variance_weighted')\n",
        "            es_mae += mean_absolute_error(test, es_fr[:len(test)])\n",
        "            \n",
        "            #########OLS Ordinary Least square method####\n",
        "            #pdb.set_trace()\n",
        "            beta = np.array([1,0.10])\n",
        "            e = np.random.normal(size=len(train))\n",
        "            X = sm.add_constant(train)\n",
        "            y = np.dot(X, beta) + e\n",
        "            model_ols = sm.OLS(y, X)\n",
        "            result_ols = model_ols.fit()\n",
        "            \n",
        "            ols_fr = result_ols.predict()\n",
        "            \n",
        "            ols_mse = mean_squared_error(test, ols_fr[:len(test)])\n",
        "            ols_rmse += sqrt(ols_mse)\n",
        "            ols_r2 += r2_score(test, ols_fr[:len(test)],  multioutput='variance_weighted')\n",
        "            ols_mae += mean_absolute_error(test, ols_fr[:len(test)])\n",
        "            \n",
        "            ############SDL #################\n",
        "            \n",
        "            tidx = pd.date_range('2016-07-01', periods=len(train), freq='H')\n",
        "            ts = pd.Series(data=train, index=tidx)\n",
        "            st = STLForecast(ts,ARIMA, model_kwargs=dict(order=(1,1,0), trend=\"t\")).fit()\n",
        "            \n",
        "            sdl_fr = st.forecast(len(test))\n",
        "            \n",
        "            ##sddl_fr.values has all the prediction\n",
        "            sdl_mse = mean_squared_error(test, sdl_fr.values)\n",
        "            sdl_rmse += sqrt(sdl_mse)\n",
        "            sdl_r2 += r2_score(test, sdl_fr.values,  multioutput='variance_weighted')\n",
        "            sdl_mae += mean_absolute_error(test, sdl_fr.values)\n",
        "            \n",
        "            ### Vector ARIMA#############\n",
        "            #pdb.set_trace()\n",
        "            df_train = pd.DataFrame({'Act1':train,\n",
        "                                     'ACT2': max(train)+np.sin(np.linspace(0, 2*np.pi, len(train)))*50})\n",
        "            df_test = pd.DataFrame({'Act1':test,\n",
        "                                    'ACT2': max(test)+np.sin(np.linspace(0, 2*np.pi, len(test)))*50})\n",
        "            \n",
        "            # fit model\n",
        "            model = VAR(df_train)\n",
        "            model_fit = model.fit()\n",
        "            # make prediction\n",
        "            yhat = model_fit.forecast(model_fit.y, steps=len(df_test))\n",
        "            \n",
        "            varma_mse = mean_squared_error(df_test.values,yhat)\n",
        "            varma_rmse += sqrt(varma_mse)\n",
        "            varma_r2 += r2_score(df_test.values,yhat, multioutput='variance_weighted')\n",
        "            varma_mae += mean_absolute_error(df_test.values,yhat)\n",
        "            \n",
        "            \n",
        "                    \n",
        "        #pdb.set_trace()\n",
        "        fn = open(\"/root/Desktop/comp-analysis-ARIMA1.txt\", 'a')\n",
        "        fn.write(str(arima_rmse/3)+\",\"+str(arima_r2/3)+\",\"+str(arima_mae/3)+\"\\n\")\n",
        "        #+\",\"+\n",
        "        #         str(es_rmse/3)+\",\"+str(es_r2/3)+\",\"+str(es_mae/3)+\",\"+\n",
        "        #         str(ols_rmse/3)+\",\"+str(ols_r2/3)+\",\"+str(ols_mae/3)+\",\"+\n",
        "        #         str(sdl_rmse/3)+\",\"+str(sdl_r2/3)+\",\"+str(sdl_mae/3)+\",\"+\n",
        "        #         str(varma_rmse/3)+\",\"+str(varma_r2/3)+\",\"+str(varma_mae/3)+\"\\n\")\n",
        "        fn.close()\n"
      ],
      "metadata": {
        "id": "PeeNW7iK0Ndx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display():\n",
        "        \n",
        "    df = pd.read_csv(\"/root/Desktop/analysis-covid.csv\")\n",
        "    c1 = df[\"arima_rmse\"]\n",
        "    c2 = df[\"es_rmse\"]\n",
        "    c3 = df[\"ols_rmse\"]\n",
        "    c4 = df[\"sdl_rmse\"]\n",
        "    c5 = df[\"varma_rmse\"]\n",
        "    data = [c1, c2, c4] #, c3, c4, c5]\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.boxplot(data, notch=True, patch_artist=True)\n",
        "    plt.xticks([1, 2, 3], [\"ARIMA\", \"ES\",\"SDL\"])\n",
        "    plt.ylabel(\"RMSE\")\n",
        "    plt.xlabel(\"Forecast Models \")\n",
        "\n",
        "    #plt.show()"
      ],
      "metadata": {
        "id": "uKOUCnXn0ZCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmkcKIPRP0LN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlbwdA-gYh25"
      },
      "source": [
        "# evaluate a give model\n",
        "def evaluate_model(model, X, y):\n",
        "  model=model.fit(X_train,y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXxHjTWuY31A"
      },
      "source": [
        "models = get_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTcjaP0EZCvf",
        "outputId": "a417b260-1e47-4a5f-bebc-8f9e4ef4142d"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWK7YbUIUzvy",
        "outputId": "1d0c1f75-1288-4dd8-d6d5-8a493de6f9c3"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[102   2]\n",
            " [ 15   3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApMDCa3qlFCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f234127-4758-4e1a-e1cf-36a653aad799"
      },
      "source": [
        "# predict probabilities\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred_prob = model.predict_proba(X_test)\n",
        "auc_score = roc_auc_score(y_test, pred_prob[:,1])\n",
        "print(\"Area Under Curve=\")\n",
        "print(auc_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under Curve=\n",
            "0.6177884615384616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcZLI5TLU3ic",
        "outputId": "8e066efd-ca2a-4d81-b01e-345c1742bf74"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "mcc=matthews_corrcoef(y_test,y_pred)\n",
        "print(\"Matthews correlation coefficient=\")\n",
        "print(mcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews correlation coefficient=\n",
            "0.2637413511922829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1lgJwGlmHUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2febf10-fb1b-476e-d1ad-3371624305c8"
      },
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "bac=balanced_accuracy_score(y_test, y_pred)\n",
        "print(\"Balanced Accuracy Score=\")\n",
        "print(bac)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Accuracy Score=\n",
            "0.5737179487179487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK0q5fZhZKGv",
        "outputId": "9bff72d8-9a36-4acb-9a8c-bfe299f7176c"
      },
      "source": [
        "from imblearn.metrics import geometric_mean_score\n",
        "gmean=geometric_mean_score(y_test, y_pred)\n",
        "print(\"Geometric Mean Score=\")\n",
        "print(gmean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Geometric Mean Score=\n",
            "0.40430377003131995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_632z4Entt-",
        "outputId": "508ae3c0-4490-4e6a-cf72-e681296044cd"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.98      0.92       104\n",
            "           1       0.60      0.17      0.26        18\n",
            "\n",
            "    accuracy                           0.86       122\n",
            "   macro avg       0.74      0.57      0.59       122\n",
            "weighted avg       0.83      0.86      0.83       122\n",
            "\n"
          ]
        }
      ]
    }
  ]
}